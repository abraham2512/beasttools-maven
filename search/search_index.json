{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Beast API Documentation Welcome to the documentation of the BEAST API Project! Getting Started to Develop! The project requires JDK v1.8 and Spark v3.0.1 for the BEAST library, ensure SPARK_HOME and JAVA_HOME are in path. Pull the repository using git clone git@github.com:abraham2512/beasttools-maven.git Maven is used to compile dependencies and launch the project Compile the project using IntelliJ with compile-time dependencies and run with StartApp as main class Project layout src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies Building Run mvn clean package to build a deployable fat-jar Download beast binaries from this link beast-bitbucket Deploying Deploy the jar with beast beast-1.1-RC.jar Additional Documentation Beast Documentation AkkaHTTP Documentation Akka Concurrency by Derek Wyatt - A great book that runs through the basics of Akka!","title":"Home"},{"location":"#welcome-to-beast-api-documentation","text":"Welcome to the documentation of the BEAST API Project!","title":"Welcome to Beast API Documentation"},{"location":"#getting-started-to-develop","text":"The project requires JDK v1.8 and Spark v3.0.1 for the BEAST library, ensure SPARK_HOME and JAVA_HOME are in path. Pull the repository using git clone git@github.com:abraham2512/beasttools-maven.git Maven is used to compile dependencies and launch the project Compile the project using IntelliJ with compile-time dependencies and run with StartApp as main class","title":"Getting Started to Develop!"},{"location":"#project-layout","text":"src/ main/ scala/ StartApp.scala # Main function and entry point to the server. actors # Contains all the actors Routes.scala # ScalaDSL routing logic. FileRegistry.scala # Helper object for the HTTP routing Actor HdfsActor.scala # Actor for indexing and partitioning of data using BEAST TileActor.scala # Actor for rendering tiles on the fly using index models DataFileDAL.scala # Relational Mapping for H2 metadata database DataFileDAO.scala utils JsonFormats.scala resources/ application.conf #Configuration File for the server pom.xml # Maven dependencies","title":"Project layout"},{"location":"#building","text":"Run mvn clean package to build a deployable fat-jar Download beast binaries from this link beast-bitbucket","title":"Building"},{"location":"#deploying","text":"Deploy the jar with beast beast-1.1-RC.jar","title":"Deploying"},{"location":"#additional-documentation","text":"Beast Documentation AkkaHTTP Documentation Akka Concurrency by Derek Wyatt - A great book that runs through the basics of Akka!","title":"Additional Documentation"},{"location":"api_example/","text":"Adding a new feature - API endpoint In this section, we will create a test endpoint named \"/meta\" to return a string for a GET request. Routes.scala This file contains the akkaHTTP routing logic and also extends the FileRegistry Actor. 1.Lets declare the controller method in the class Routes. def getMeta: Future[String] = tileActor ? GetMetaData Sends GetMetaData message to the Tile Actor. ? is the operator to ask an actor, and it expects an asynchronous reply. The return type of the function is a Future of type [String] 2.Append akkaHTTP routing directive in var fileRoutes Add the route with the below syntax. Ensure to use ~ at the end to append underlying route directives. val fileRoutes: Route = { ... ... ... CORS and Failure Handling functions ... cors(){ handleErrors { pathPrefix(\"meta\") { pathEnd { get{ complete(StatusCodes.OK,getMeta) } } } ~ ... ... ... \"/tiles/\" and \"/files/\" endpoints ... } } } TileActor.scala 3.Now lets define a behaviour for the GetMetaData message in the TileActor In the TileActor object, create case class for the behavior and extend TileCommand trait. final case class GetMetaData(replyTo: ActorRef[String]) extends TileCommand 4.In the apply() method of the object, add the following case to Behaviors.ReceiveMessage {} method def apply(): Behavior[TileCommand] = Behaviors.setup { context: ActorContext[TileCommand] => Behaviors.receiveMessage { case GetMetaData(replyTo) => //GET METADATA replyTo ! \"My new BeastApi endpoint!\" Behaviors.same } } Lets run a curl test curl http://localhost:8080/meta Response: Status 200 My new BeastApi endpoint!","title":"Example1 /meta/"},{"location":"api_example/#adding-a-new-feature-api-endpoint","text":"In this section, we will create a test endpoint named \"/meta\" to return a string for a GET request.","title":"Adding a new feature - API endpoint"},{"location":"api_example/#routesscala","text":"This file contains the akkaHTTP routing logic and also extends the FileRegistry Actor. 1.Lets declare the controller method in the class Routes. def getMeta: Future[String] = tileActor ? GetMetaData Sends GetMetaData message to the Tile Actor. ? is the operator to ask an actor, and it expects an asynchronous reply. The return type of the function is a Future of type [String] 2.Append akkaHTTP routing directive in var fileRoutes Add the route with the below syntax. Ensure to use ~ at the end to append underlying route directives. val fileRoutes: Route = { ... ... ... CORS and Failure Handling functions ... cors(){ handleErrors { pathPrefix(\"meta\") { pathEnd { get{ complete(StatusCodes.OK,getMeta) } } } ~ ... ... ... \"/tiles/\" and \"/files/\" endpoints ... } } }","title":"Routes.scala"},{"location":"api_example/#tileactorscala","text":"3.Now lets define a behaviour for the GetMetaData message in the TileActor In the TileActor object, create case class for the behavior and extend TileCommand trait. final case class GetMetaData(replyTo: ActorRef[String]) extends TileCommand 4.In the apply() method of the object, add the following case to Behaviors.ReceiveMessage {} method def apply(): Behavior[TileCommand] = Behaviors.setup { context: ActorContext[TileCommand] => Behaviors.receiveMessage { case GetMetaData(replyTo) => //GET METADATA replyTo ! \"My new BeastApi endpoint!\" Behaviors.same } } Lets run a curl test curl http://localhost:8080/meta Response: Status 200 My new BeastApi endpoint!","title":"TileActor.scala"},{"location":"backend/","text":"Akka HTTP Endpoints POST /files Starts the download for a file if it doesnt exist. curl -X POST http://127.0.0.1:8080/files -d '{\"filename\": \"SafetyDept\",\"filetype\": \"shapefile\",\"filesource\": \"/some/path/to/file\",\"filestatus\": \"start\"}' Sample Response Status 201 - Created { \"description\": \"created\" } GET /files Return all the files in our server curl -X GET http://127.0.0.1:8080/files Sample Response Status 200 - OK { \"files\": [ { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"default\" }, { \"filename\": \"Sections\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } ] } GET /files/{id} Returns the details of a file curl -X GET http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } DELETE /files/{id} Deletes the dataset from server curl -X DELETE http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"description\": \"deleted\" } GET /tiles/ Returns pre generated tile or generates one on the fly. curl -X GET http://127.0.0.1:8080/tiles?dataset=<D>&z=<Z>&x=<X>&y=<Y> Returns a tile image for the dataset D with coordinates Z, X, Y","title":"Backend"},{"location":"backend/#akka-http-endpoints","text":"","title":"Akka HTTP Endpoints"},{"location":"backend/#post-files","text":"Starts the download for a file if it doesnt exist. curl -X POST http://127.0.0.1:8080/files -d '{\"filename\": \"SafetyDept\",\"filetype\": \"shapefile\",\"filesource\": \"/some/path/to/file\",\"filestatus\": \"start\"}' Sample Response Status 201 - Created { \"description\": \"created\" }","title":"POST /files"},{"location":"backend/#get-files","text":"Return all the files in our server curl -X GET http://127.0.0.1:8080/files Sample Response Status 200 - OK { \"files\": [ { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"default\" }, { \"filename\": \"Sections\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" } ] }","title":"GET /files"},{"location":"backend/#get-filesid","text":"Returns the details of a file curl -X GET http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"filename\": \"SafetyDept\", \"filesource\": \"/some/path/to/file\", \"filestatus\": \"indexed\", \"filetype\": \"shapefile\" }","title":"GET /files/{id}"},{"location":"backend/#delete-filesid","text":"Deletes the dataset from server curl -X DELETE http://127.0.0.1:8080/files/SafetyDept Sample Response Status 200 - OK { \"description\": \"deleted\" }","title":"DELETE /files/{id}"},{"location":"backend/#get-tiles","text":"Returns pre generated tile or generates one on the fly. curl -X GET http://127.0.0.1:8080/tiles?dataset=<D>&z=<Z>&x=<X>&y=<Y> Returns a tile image for the dataset D with coordinates Z, X, Y","title":"GET /tiles/"},{"location":"frontend/","text":"Maps Interface The interface is built with OpenLayers and JavaScript. Start your own front-end OpenLayers.js can be imported as a browser script but modern JavaScript works best when using and authoring modules. The recommended way of using OpenLayers is installing the ol package. This can be done through node using the command npx create-ol-app . For more information, see the guide here Clone Beast Viz To clone the default front-end, run this command to clone the repository git clone git@github.com:abraham2512/beasttools-maven.git and open the frontend/beastol/ project folder. The project has index.js , index.html , styles.css and a package.json configuration file. Setup the interface First install node modules from package.json with npm install . For development run the server using npm run start . For deployment bundle the files using npm run build to create one index.html file that can be served. index.js function launchMap(filename) :Creates a map with the filename ID. :Makes GET request to /tiles/ endpoint in backend with dataset = {filename} and {Z}, {X}, {Y} values. function handleDataFileSubmit(event) :Handles form submit with data json. :Makes POST request to /files/ endpoint with data payload. This triggers partitioning and indexing of data. function appendCardDiv(filename) :Appends a new Card for the dataset in the UI to track its progress and launch when ready. index.html This file imports the index.js module and contains the main html.","title":"Frontend"},{"location":"frontend/#maps-interface","text":"The interface is built with OpenLayers and JavaScript.","title":"Maps Interface"},{"location":"frontend/#start-your-own-front-end","text":"OpenLayers.js can be imported as a browser script but modern JavaScript works best when using and authoring modules. The recommended way of using OpenLayers is installing the ol package. This can be done through node using the command npx create-ol-app . For more information, see the guide here","title":"Start your own front-end"},{"location":"frontend/#clone-beast-viz","text":"To clone the default front-end, run this command to clone the repository git clone git@github.com:abraham2512/beasttools-maven.git and open the frontend/beastol/ project folder. The project has index.js , index.html , styles.css and a package.json configuration file.","title":"Clone Beast Viz"},{"location":"frontend/#setup-the-interface","text":"First install node modules from package.json with npm install . For development run the server using npm run start . For deployment bundle the files using npm run build to create one index.html file that can be served.","title":"Setup the interface"},{"location":"frontend/#indexjs","text":"","title":"index.js"},{"location":"frontend/#function-launchmapfilename","text":":Creates a map with the filename ID. :Makes GET request to /tiles/ endpoint in backend with dataset = {filename} and {Z}, {X}, {Y} values.","title":"function launchMap(filename)"},{"location":"frontend/#function-handledatafilesubmitevent","text":":Handles form submit with data json. :Makes POST request to /files/ endpoint with data payload. This triggers partitioning and indexing of data.","title":"function handleDataFileSubmit(event)"},{"location":"frontend/#function-appendcarddivfilename","text":":Appends a new Card for the dataset in the UI to track its progress and launch when ready.","title":"function appendCardDiv(filename)"},{"location":"frontend/#indexhtml","text":"This file imports the index.js module and contains the main html.","title":"index.html"},{"location":"h2db_example/","text":"Adding a new feature H2DB H2 database is used as a metadata store in our server to track the status of Datasets. The database is connected with JDBC and all actors importing the driver will have access to it. We can define entity relationships using a Functional Relational Mapping (FRM) using Scala Lang Integrated Connection Kit or Slick It allows you to work with stored data almost as if you were using Scala collections while at the same time giving you full control over when database access happens and which data is transferred. You can also use SQL directly. Execution of database actions is done asynchronously, making Slick a perfect fit for reactive applications based on Akka. In the Models folder, there are two files DataFileDAO.scala which contains the entity expressions and the accessor methods. The access methods to these entities are in the DataFileDAL.scala . Some of the methods have an Await keyword to enforce a synchronous response when accessing the database. You can define your new database function in this file and access it in the other actors.","title":"Example3 Accessing H2DB"},{"location":"h2db_example/#adding-a-new-feature-h2db","text":"H2 database is used as a metadata store in our server to track the status of Datasets. The database is connected with JDBC and all actors importing the driver will have access to it. We can define entity relationships using a Functional Relational Mapping (FRM) using Scala Lang Integrated Connection Kit or Slick It allows you to work with stored data almost as if you were using Scala collections while at the same time giving you full control over when database access happens and which data is transferred. You can also use SQL directly. Execution of database actions is done asynchronously, making Slick a perfect fit for reactive applications based on Akka. In the Models folder, there are two files DataFileDAO.scala which contains the entity expressions and the accessor methods. The access methods to these entities are in the DataFileDAL.scala . Some of the methods have an Await keyword to enforce a synchronous response when accessing the database. You can define your new database function in this file and access it in the other actors.","title":"Adding a new feature H2DB"},{"location":"ol_example/","text":"Adding a new feature - OpenLayers event In this section we use a openlayers click event to create a popup handler. First we create the HTML elements that make the popup inside the map_div in index.html . <div id=\"map_div\" class=\"child2\"> <div id=\"popup\" class=\"ol-popup\"> <a href=\"#\" id=\"popup-closer\" class=\"ol-popup-closer\"></a> <div id=\"popup-content\"></div> </div> <div id=\"map\"></div> Now we can bind an event handler for a 'singleClick' event in OpenLayers. In launchMap() function in OpenLayers, first we define the characteristics of the popup. /** * Elements that make up the popup. */ const container = document.getElementById('popup'); const content = document.getElementById('popup-content'); const closer = document.getElementById('popup-closer'); /** * Create an overlay to anchor the popup to the map. */ const overlay = new Overlay({ element: container, autoPan: { animation: { duration: 250, }, }, }); /** * Add a click handler to hide the popup. * @return {boolean} Don't follow the href. */ closer.onclick = function () { overlay.setPosition(undefined); closer.blur(); return false; }; Now we can bind an event handler to this popup function /** * Add a click handler to the map to render the popup. */ map.on('singleclick', function (evt) { let coordinate = evt.coordinate; const hdms = toStringHDMS(toLonLat(coordinate)); content.innerHTML = '<p>You clicked here:</p><code>' + hdms + '</code>'; axios.get(`http://127.0.0.1:8080/meta`) .then( function(response){ let data = response.data; console.log(data); content.innerHTML = '<p>This is the data:</p><code>' + data + '</code>'; }) .catch( function(error){ console.log(error); } ); overlay.setPosition(coordinate); });","title":"Example2 OL event"},{"location":"ol_example/#adding-a-new-feature-openlayers-event","text":"In this section we use a openlayers click event to create a popup handler. First we create the HTML elements that make the popup inside the map_div in index.html . <div id=\"map_div\" class=\"child2\"> <div id=\"popup\" class=\"ol-popup\"> <a href=\"#\" id=\"popup-closer\" class=\"ol-popup-closer\"></a> <div id=\"popup-content\"></div> </div> <div id=\"map\"></div> Now we can bind an event handler for a 'singleClick' event in OpenLayers. In launchMap() function in OpenLayers, first we define the characteristics of the popup. /** * Elements that make up the popup. */ const container = document.getElementById('popup'); const content = document.getElementById('popup-content'); const closer = document.getElementById('popup-closer'); /** * Create an overlay to anchor the popup to the map. */ const overlay = new Overlay({ element: container, autoPan: { animation: { duration: 250, }, }, }); /** * Add a click handler to hide the popup. * @return {boolean} Don't follow the href. */ closer.onclick = function () { overlay.setPosition(undefined); closer.blur(); return false; }; Now we can bind an event handler to this popup function /** * Add a click handler to the map to render the popup. */ map.on('singleclick', function (evt) { let coordinate = evt.coordinate; const hdms = toStringHDMS(toLonLat(coordinate)); content.innerHTML = '<p>You clicked here:</p><code>' + hdms + '</code>'; axios.get(`http://127.0.0.1:8080/meta`) .then( function(response){ let data = response.data; console.log(data); content.innerHTML = '<p>This is the data:</p><code>' + data + '</code>'; }) .catch( function(error){ console.log(error); } ); overlay.setPosition(coordinate); });","title":"Adding a new feature - OpenLayers event"},{"location":"overview/","text":"Beast Microservice The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model and message passing. It uses the functional programming paradigm which is ideal for using with Spark and BEAST. Akka and AkkaHTTP Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model) By building the endpoints using actors and separating the indexing/partitioning jobs and tile generation jobs, we can achieve a degree of parallelism in our API. Our project has three Actors Actors in BeastAPI Routes.scala : Entry point and handles routing logic. Passes messages to TileActor and HdfsActor HdfsActor.scala : Partitioning and indexing job messages are received and performed by this Actor TileActor.scala : Tiles are generated on the fly from indexes by this actor The actors can be deployed and scaled horizontally into the cluster using the Akka Cluster API. The following video is a good source of motivation to use Akka-Cluster Current Architecture design Using Akka ActorContext and Behaviors A Scala object can be made into an Akka actor simply by importing akka.actor.typed.scaladsl.ActorContext and akka.actor.typed.scaladsl.Behaviors into your scala object. Use Behaviours.setup to create a match case for ActorContext . Any message passed to this actor will be processed depending on its match case. A simple example would be object MyActor{ def apply(): Behavior[CommandTrait] = Behaviors.setup { context: ActorContext[CommandTrait] => println(\"actors.HdfsRegistry: Hdfs Actor awake\") Behaviors.receiveMessage { case SpeakText(msg) => println(s\"actors.HdfsActor: got a msg: $msg\") Behaviors.same case _ => println(\"Default Case\") Behaviors.same } } } For the Actor to be visible to other actors in the server, we can register it to the Receptionist . Something like this would tell the receptionist to register the actor. context.system.receptionist ! Receptionist.Register(\"KeyForNewActor\", context.self) More information on this can be found in this guide akka-docs","title":"Overview"},{"location":"overview/#beast-microservice","text":"The microservice backend is to enable upload and processing of Spatial-Temporal data files on the Hadoop using Spark and BEAST library. There were a few design considerations and AkkaHTTP was deemed a good fit for the API. It is a lightweight but feature-rich and highly scalable toolkit for building API endpoints based on the Actor Model and message passing. It uses the functional programming paradigm which is ideal for using with Spark and BEAST.","title":"Beast Microservice"},{"location":"overview/#akka-and-akkahttp","text":"Concurrency through message passing (Actors) Non-blocking by default (Futures) Fault Tolerance (a.k.a. Resiliency, \u2018let it crash\u2019 model) By building the endpoints using actors and separating the indexing/partitioning jobs and tile generation jobs, we can achieve a degree of parallelism in our API. Our project has three Actors","title":"Akka and AkkaHTTP"},{"location":"overview/#actors-in-beastapi","text":"Routes.scala : Entry point and handles routing logic. Passes messages to TileActor and HdfsActor HdfsActor.scala : Partitioning and indexing job messages are received and performed by this Actor TileActor.scala : Tiles are generated on the fly from indexes by this actor The actors can be deployed and scaled horizontally into the cluster using the Akka Cluster API. The following video is a good source of motivation to use Akka-Cluster","title":"Actors in BeastAPI"},{"location":"overview/#current-architecture-design","text":"","title":"Current Architecture design"},{"location":"overview/#using-akka-actorcontext-and-behaviors","text":"A Scala object can be made into an Akka actor simply by importing akka.actor.typed.scaladsl.ActorContext and akka.actor.typed.scaladsl.Behaviors into your scala object. Use Behaviours.setup to create a match case for ActorContext . Any message passed to this actor will be processed depending on its match case. A simple example would be object MyActor{ def apply(): Behavior[CommandTrait] = Behaviors.setup { context: ActorContext[CommandTrait] => println(\"actors.HdfsRegistry: Hdfs Actor awake\") Behaviors.receiveMessage { case SpeakText(msg) => println(s\"actors.HdfsActor: got a msg: $msg\") Behaviors.same case _ => println(\"Default Case\") Behaviors.same } } } For the Actor to be visible to other actors in the server, we can register it to the Receptionist . Something like this would tell the receptionist to register the actor. context.system.receptionist ! Receptionist.Register(\"KeyForNewActor\", context.self) More information on this can be found in this guide akka-docs","title":"Using Akka ActorContext and Behaviors"}]}